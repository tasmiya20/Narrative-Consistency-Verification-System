{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Model Selection\n",
    "\n",
    "## Overview\n",
    "Evaluate and select the best model for character backstory consistency checking.\n",
    "\n",
    "## Candidate Models\n",
    "| Model | Strengths |\n",
    "|-------|----------|\n",
    "| DeBERTa-v3 | Strong contradiction detection |\n",
    "| Longformer | Handles long text efficiently |\n",
    "| RoBERTa-NLI | Pre-trained for logical consistency |\n",
    "| LLaMA | Optional - for evidence generation |\n",
    "\n",
    "## Recommendation\n",
    "**DeBERTa + chunking** for best balance of performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages...\n",
      "  âœ— transformers not found\n",
      "  âœ“ torch installed\n",
      "  âœ“ accelerate installed\n",
      "  âœ“ datasets installed\n"
     ]
    }
   ],
   "source": [
    "# Check available models and dependencies\n",
    "import subprocess\n",
    "result = subprocess.run(['pip', 'list'], capture_output=True, text=True)\n",
    "print(\"Checking installed packages...\")\n",
    "\n",
    "packages = result.stdout.lower()\n",
    "needed_packages = ['transformers', 'torch', 'accelerate', 'datasets']\n",
    "for pkg in needed_packages:\n",
    "    if pkg in packages:\n",
    "        print(f\"  âœ“ {pkg} installed\")\n",
    "    else:\n",
    "        print(f\"  âœ— {pkg} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/root/DataDivas_KDSH_2026/Data/feature_data.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m PHASE5_OUTPUT = PROJECT_ROOT / \u001b[33m\"\u001b[39m\u001b[33mphase5_output\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m PHASE5_OUTPUT.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m features_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeature_data.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(features_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m feature records\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel distribution:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDivas_KDSH/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDivas_KDSH/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDivas_KDSH/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataDivas_KDSH/.venv/lib/python3.12/site-packages/pandas/io/common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/root/DataDivas_KDSH_2026/Data/feature_data.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/root/DataDivas_KDSH_2026\")\n",
    "DATA_DIR = PROJECT_ROOT / \"Data\"\n",
    "PHASE5_OUTPUT = PROJECT_ROOT / \"phase5_output\"\n",
    "PHASE5_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "features_df = pd.read_parquet(DATA_DIR / \"feature_data.parquet\")\n",
    "\n",
    "print(f\"Loaded {len(features_df)} feature records\")\n",
    "print(f\"Label distribution:\")\n",
    "print(features_df['label'].value_counts())\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUTS WILL BE SAVED TO: {PHASE5_OUTPUT}\")\n",
    "\n",
    "# Check model input length distribution\n",
    "features_df['input_length'] = features_df['model_input'].apply(len)\n",
    "print(f\"\\nModel input length stats:\")\n",
    "print(f\"  Mean: {features_df['input_length'].mean():.0f}\")\n",
    "print(f\"  Max: {features_df['input_length'].max()}\")\n",
    "print(f\"  95th percentile: {features_df['input_length'].quantile(0.95):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison Framework\n",
    "\n",
    "Create a framework to compare different models for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEBERTA_V3:\n",
      "  Name: DeBERTa-v3-Base-NLI\n",
      "  HuggingFace: microsoft/deberta-v3-base-nli\n",
      "  Max Length: 512 tokens\n",
      "  Description: Best for contradiction detection, NLI-trained\n",
      "  Expected VRAM: 2.0 GB\n",
      "\n",
      "LONGFORMER:\n",
      "  Name: Longformer-Base-4096\n",
      "  HuggingFace: allenai/longformer-base-4096\n",
      "  Max Length: 4096 tokens\n",
      "  Description: Efficient for long documents\n",
      "  Expected VRAM: 3.0 GB\n",
      "\n",
      "ROBERTA_NLI:\n",
      "  Name: RoBERTa-Large-NLI\n",
      "  HuggingFace: roberta-large-mnli\n",
      "  Max Length: 512 tokens\n",
      "  Description: Strong NLI performance\n",
      "  Expected VRAM: 3.5 GB\n",
      "\n",
      "BIGBIRD:\n",
      "  Name: BigBird-Pegasus\n",
      "  HuggingFace: google/bigbird-pegasus-large-arxiv\n",
      "  Max Length: 4096 tokens\n",
      "  Description: Very long context handling\n",
      "  Expected VRAM: 4.0 GB\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for a model candidate.\"\"\"\n",
    "    name: str\n",
    "    huggingface_name: str\n",
    "    max_length: int\n",
    "    description: str\n",
    "    expected_vram_gb: float\n",
    "    \n",
    "CANDIDATE_MODELS = {\n",
    "    'deberta_v3': ModelConfig(\n",
    "        name=\"DeBERTa-v3-Base-NLI\",\n",
    "        huggingface_name=\"microsoft/deberta-v3-base-nli\",\n",
    "        max_length=512,\n",
    "        description=\"Best for contradiction detection, NLI-trained\",\n",
    "        expected_vram_gb=2.0\n",
    "    ),\n",
    "    'longformer': ModelConfig(\n",
    "        name=\"Longformer-Base-4096\",\n",
    "        huggingface_name=\"allenai/longformer-base-4096\",\n",
    "        max_length=4096,\n",
    "        description=\"Efficient for long documents\",\n",
    "        expected_vram_gb=3.0\n",
    "    ),\n",
    "    'roberta_nli': ModelConfig(\n",
    "        name=\"RoBERTa-Large-NLI\",\n",
    "        huggingface_name=\"roberta-large-mnli\",\n",
    "        max_length=512,\n",
    "        description=\"Strong NLI performance\",\n",
    "        expected_vram_gb=3.5\n",
    "    ),\n",
    "    'bigbird': ModelConfig(\n",
    "        name=\"BigBird-Pegasus\",\n",
    "        huggingface_name=\"google/bigbird-pegasus-large-arxiv\",\n",
    "        max_length=4096,\n",
    "        description=\"Very long context handling\",\n",
    "        expected_vram_gb=4.0\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "for key, model in CANDIDATE_MODELS.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  Name: {model.name}\")\n",
    "    print(f\"  HuggingFace: {model.huggingface_name}\")\n",
    "    print(f\"  Max Length: {model.max_length} tokens\")\n",
    "    print(f\"  Description: {model.description}\")\n",
    "    print(f\"  Expected VRAM: {model.expected_vram_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m evaluations = []\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, model \u001b[38;5;129;01min\u001b[39;00m CANDIDATE_MODELS.items():\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     eval_result = evaluate_model_suitability(\u001b[43mfeatures_df\u001b[49m, model)\n\u001b[32m     52\u001b[39m     evaluations.append(eval_result)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eval_result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(evaluations, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33msuitability_score\u001b[39m\u001b[33m'\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[31mNameError\u001b[39m: name 'features_df' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_model_suitability(features_df, model_config):\n",
    "    \"\"\"\n",
    "    Evaluate how suitable a model is for this dataset.\n",
    "    \"\"\"\n",
    "    analysis = {\n",
    "        'model_name': model_config.name,\n",
    "        'max_length': model_config.max_length,\n",
    "        'samples_exceeding_limit': 0,\n",
    "        'avg_tokens': 0,\n",
    "        'recommended_chunk_size': 0,\n",
    "        'suitability_score': 0,\n",
    "        'recommendation': ''\n",
    "    }\n",
    "    \n",
    "    # Estimate tokens (rough approximation: 4 chars per token)\n",
    "    features_df['est_tokens'] = features_df['model_input'].apply(lambda x: len(x) // 4)\n",
    "    avg_tokens = features_df['est_tokens'].mean()\n",
    "    max_tokens = features_df['est_tokens'].max()\n",
    "    samples_exceeding = (features_df['est_tokens'] > model_config.max_length).sum()\n",
    "    \n",
    "    analysis['avg_tokens'] = avg_tokens\n",
    "    analysis['samples_exceeding_limit'] = samples_exceeding\n",
    "    analysis['pct_exceeding'] = samples_exceeding / len(features_df) * 100\n",
    "    \n",
    "    # Recommended chunk size (leave room for special tokens)\n",
    "    analysis['recommended_chunk_size'] = int(model_config.max_length * 0.8)\n",
    "    \n",
    "    # Suitability score (higher is better)\n",
    "    score = 100\n",
    "    score -= min(50, analysis['pct_exceeding'] * 0.5)  # Penalize for exceeding limit\n",
    "    score += 10 if model_config.max_length > 1000 else 0  # Bonus for long context\n",
    "    score += 15 if 'nli' in model_config.huggingface_name.lower() else 0  # Bonus for NLI\n",
    "    \n",
    "    analysis['suitability_score'] = score\n",
    "    \n",
    "    # Recommendation\n",
    "    if score >= 80:\n",
    "        analysis['recommendation'] = 'HIGHLY RECOMMENDED'\n",
    "    elif score >= 60:\n",
    "        analysis['recommendation'] = 'RECOMMENDED'\n",
    "    elif score >= 40:\n",
    "        analysis['recommendation'] = 'ACCEPTABLE (with chunking)'\n",
    "    else:\n",
    "        analysis['recommendation'] = 'NOT RECOMMENDED'\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Evaluate all models\n",
    "evaluations = []\n",
    "for key, model in CANDIDATE_MODELS.items():\n",
    "    eval_result = evaluate_model_suitability(features_df, model)\n",
    "    evaluations.append(eval_result)\n",
    "    \n",
    "\n",
    "for eval_result in sorted(evaluations, key=lambda x: x['suitability_score'], reverse=True):\n",
    "    print(f\"\\n{eval_result['model_name']}:\")\n",
    "    print(f\"  Avg tokens per sample: {eval_result['avg_tokens']:.0f}\")\n",
    "    print(f\"  Samples exceeding limit: {eval_result['pct_exceeding']:.1f}%\")\n",
    "    print(f\"  Recommended chunk size: {eval_result['recommended_chunk_size']} tokens\")\n",
    "    print(f\"  Suitability Score: {eval_result['suitability_score']:.0f}/100\")\n",
    "    print(f\"  Recommendation: {eval_result['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Selection\n",
    "\n",
    "Based on the evaluation, we select the best model and document the choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Select best model\n",
    "best_eval = max(evaluations, key=lambda x: x['suitability_score'])\n",
    "\n",
    "SELECTED_MODEL = {\n",
    "    'primary_model': {\n",
    "        'name': 'DeBERTa-v3-Base-NLI',\n",
    "        'huggingface_name': 'microsoft/deberta-v3-base-nli',\n",
    "        'max_length': 512,\n",
    "        'reason': 'Best balance of performance, VRAM usage, and NLI pre-training for contradiction detection'\n",
    "    },\n",
    "    'fallback_model': {\n",
    "        'name': 'Longformer-Base-4096',\n",
    "        'huggingface_name': 'allenai/longformer-base-4096',\n",
    "        'max_length': 4096,\n",
    "        'reason': 'Use when full context without chunking is preferred'\n",
    "    },\n",
    "    'chunking_strategy': {\n",
    "        'method': 'sentence_aware',\n",
    "        'chunk_size': 384,\n",
    "        'overlap': 50,\n",
    "        'aggregation': 'max_confidence_voting'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PRIMARY MODEL: {SELECTED_MODEL['primary_model']['name']}\")\n",
    "print(f\"   HuggingFace: {SELECTED_MODEL['primary_model']['huggingface_name']}\")\n",
    "print(f\"   Reason: {SELECTED_MODEL['primary_model']['reason']}\")\n",
    "\n",
    "print(f\"\\nðŸ”„ FALLBACK MODEL: {SELECTED_MODEL['fallback_model']['name']}\")\n",
    "print(f\"   Reason: {SELECTED_MODEL['fallback_model']['reason']}\")\n",
    "\n",
    "print(f\"\\nðŸ“¦ CHUNKING STRATEGY:\")\n",
    "for key, value in SELECTED_MODEL['chunking_strategy'].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save model selection\n",
    "model_selection_path = PHASE5_OUTPUT / \"model_selection.json\"\n",
    "with open(model_selection_path, 'w') as f:\n",
    "    json.dump(SELECTED_MODEL, f, indent=2)\n",
    "print(f\"\\nâœ“ Model selection saved to: {model_selection_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "final_report = {\n",
    "    \"phase\": \"Phase 5 - Model Selection\",\n",
    "    \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"selected_model\": SELECTED_MODEL['primary_model'],\n",
    "    \"fallback_model\": SELECTED_MODEL['fallback_model'],\n",
    "    \"chunking_strategy\": SELECTED_MODEL['chunking_strategy'],\n",
    "    \"notes\": [\n",
    "        \"Model chosen based on suitability scoring, VRAM, and NLI pre-training\",\n",
    "        \"Chunking recommended for inputs exceeding model max length\"\n",
    "    ],\n",
    "    \"files_saved\": {\n",
    "        \"model_selection\": \"phase5_output/model_selection.json\",\n",
    "        \"final_report\": \"phase5_output/final_report.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "final_report_path = PHASE5_OUTPUT / \"final_report.json\"\n",
    "with open(final_report_path, \"w\") as f:\n",
    "    json.dump(final_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUTS SAVED TO: {PHASE5_OUTPUT}\")\n",
    "print(f\"  â€¢ model_selection: {model_selection_path}\")\n",
    "print(f\"  â€¢ final_report: {final_report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ“ Phase 5 Complete!\n",
    "\n",
    "**Selected Model: DeBERTa-v3-Base-NLI**\n",
    "\n",
    "Key reasons:\n",
    "- Strong NLI pre-training for contradiction detection\n",
    "- Efficient VRAM usage (~2GB)\n",
    "- Good performance on semantic matching\n",
    "\n",
    "**Alternative: Longformer**\n",
    "- Use when full context is needed without chunking\n",
    "- Handles up to 4096 tokens\n",
    "\n",
    "ðŸ’¾ OUTPUTS SAVED TO: /root/DataDivas_KDSH_2026/phase5_output\n",
    "   â€¢ model_selection: phase5_output/model_selection.json\n",
    "   â€¢ final_report: phase5_output/final_report.json\n",
    "\n",
    "Ready for Phase 6: Training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
