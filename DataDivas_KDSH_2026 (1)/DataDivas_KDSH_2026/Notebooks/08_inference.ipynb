{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8: Inference & System Pipeline\n",
    "\n",
    "Complete end-to-end system for character backstory consistency checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "PROJECT_ROOT = Path(\"/root/DataDivas_KDSH_2026\")\n",
    "DATA_DIR = PROJECT_ROOT / \"Data\"\n",
    "PHASE5_OUTPUT = PROJECT_ROOT / \"phase5_output\"\n",
    "PHASE6_OUTPUT = PROJECT_ROOT / \"phase6_output\"\n",
    "\n",
    "# Load model selection (prefer phase5_output)\n",
    "model_selection_path = PHASE5_OUTPUT / \"model_selection.json\"\n",
    "if not model_selection_path.exists():\n",
    "    model_selection_path = DATA_DIR / \"model_selection.json\"\n",
    "\n",
    "with open(model_selection_path) as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "model_name = model_config['primary_model']['huggingface_name']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Prefer model from phase6, then phase5, then Data\n",
    "best_model_path = PHASE6_OUTPUT / \"best_model.pt\"\n",
    "if not best_model_path.exists():\n",
    "    best_model_path = PHASE5_OUTPUT / \"best_model.pt\"\n",
    "if not best_model_path.exists():\n",
    "    best_model_path = DATA_DIR / \"best_model.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Prefer training results from phase6 then Data\n",
    "training_results_path = PHASE6_OUTPUT / \"training_results.json\"\n",
    "if not training_results_path.exists():\n",
    "    training_results_path = DATA_DIR / \"training_results.json\"\n",
    "if not training_results_path.exists():\n",
    "    training_results_path = PHASE5_OUTPUT / \"training_results.json\"\n",
    "with open(training_results_path) as f:\n",
    "    training_results = json.load(f)\n",
    "\n",
    "print(\"System loaded. Model:\", model_name, \"F1:\", training_results.get('best_f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterConsistencyChecker:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "\n",
    "    def chunk_text(self, text, chunk_size=384):\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunks.append(\" \".join(words[i:i + chunk_size]))\n",
    "        return chunks\n",
    "\n",
    "    def classify(self, backstory, chunk_text):\n",
    "        input_text = \"[CLS] \" + backstory + \" [SEP] \" + chunk_text + \" [SEP]\"\n",
    "        encoding = self.tokenizer(input_text, truncation=True, max_length=512, padding='max_length', return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            confidence = torch.max(probs).item()\n",
    "            prediction = torch.argmax(probs, dim=1).item()\n",
    "        return {'prediction': prediction, 'confidence': confidence}\n",
    "\n",
    "    def check_consistency(self, character_name, backstory, full_text):\n",
    "        chunks = self.chunk_text(full_text)\n",
    "        chunk_results = [self.classify(backstory, chunk) for chunk in chunks]\n",
    "        consistent_count = sum(1 for r in chunk_results if r['prediction'] == 1)\n",
    "        contradict_count = len(chunk_results) - consistent_count\n",
    "        avg_confidence = sum(r['confidence'] for r in chunk_results) / len(chunk_results)\n",
    "        final_prediction = 1 if consistent_count > contradict_count else 0\n",
    "        label = \"CONSISTENT\" if final_prediction == 1 else \"CONTRADICT\"\n",
    "        return {\n",
    "            'character_name': character_name,\n",
    "            'label': final_prediction,\n",
    "            'label_text': label,\n",
    "            'confidence': avg_confidence,\n",
    "            'chunk_summary': {'consistent': consistent_count, 'contradict': contradict_count}\n",
    "        }\n",
    "\n",
    "checker = CharacterConsistencyChecker(model, tokenizer, device)\n",
    "print(\"Checker ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_parquet(DATA_DIR / \"feature_data.parquet\")\n",
    "print(\"Running inference on\", len(test_df), \"samples...\")\n",
    "\n",
    "inference_results = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    result = checker.check_consistency(row['character_name'], row['backstory'], row['chunk_text'])\n",
    "    inference_results.append({\n",
    "        'entry_id': row['entry_id'],\n",
    "        'character_name': result['character_name'],\n",
    "        'label': result['label'],\n",
    "        'label_text': result['label_text'],\n",
    "        'confidence': result['confidence'],\n",
    "        'consistent_chunks': result['chunk_summary']['consistent'],\n",
    "        'contradict_chunks': result['chunk_summary']['contradict']\n",
    "    })\n",
    "\n",
    "print(\"Inference complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(inference_results)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "RESULTS\n",
    "=\" * 50)\n",
    "print(\"Total:\", len(results_df))\n",
    "print(\"Label distribution:\")\n",
    "for label, count in results_df['label_text'].value_counts().items():\n",
    "    print(\"  \", label, \":\", count)\n",
    "print(\"Confidence: mean=\", round(results_df['confidence'].mean(), 2), \"max=\", round(results_df['confidence'].max(), 2))\n",
    "\n",
    "results_df.to_csv(DATA_DIR / \"final_predictions.csv\", index=False)\n",
    "results_df[['entry_id', 'label', 'confidence']].to_csv(DATA_DIR / \"submission.csv\", index=False)\n",
    "\n",
    "print(\"\\nOutputs saved:\")\n",
    "print(\"  - final_predictions.csv\")\n",
    "print(\"  - submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Complete\n",
    "\n",
    "All 8 phases implemented:\n",
    "\n",
    "1. Data Ingestion - Pathway tables\n",
    "2. Character Extraction - Name matching\n",
    "3. Chunking Strategy - Sentence-aware\n",
    "4. Feature Engineering - Semantic signals\n",
    "5. Model Selection - DeBERTa-v3-NLI\n",
    "6. Training - Binary classifier\n",
    "7. Evidence Generation - Pattern-based\n",
    "8. Inference Pipeline - End-to-end\n",
    "\n",
    "Output: final_predictions.csv, submission.csv, best_model.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
