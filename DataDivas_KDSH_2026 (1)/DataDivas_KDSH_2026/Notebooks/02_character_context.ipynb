{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2: CHARACTER CONTEXT EXTRACTION & EVIDENCE CONSTRUCTION\n",
      "================================================================================\n",
      "Goal: Isolate narrative evidence relevant to character development\n",
      "      while preserving causal and emotional context.\n",
      "================================================================================\n",
      "\n",
      "1. Recreating Phase 1 environment...\n",
      "   Project root: /root/DataDivas_KDSH_2026\n",
      "   Data directory: /root/DataDivas_KDSH_2026/Data\n",
      "   Books directory: /root/DataDivas_KDSH_2026/Data/Books\n",
      "\n",
      "2. Recreating Phase 1 tables...\n",
      "   Loaded: The Count of Monte Cristo - 464,020 words\n",
      "   Loaded: In search of the castaways - 138,830 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08T16:26:08]:INFO:Preparing Pathway computation\n",
      "[2026-01-08T16:26:08]:INFO:Enter read_snapshot method with reader PosixLike\n",
      "[2026-01-08T16:26:08]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:08]:INFO:subscribe-0: Done writing 0 entries, time 1767889568374. Current batch writes took: 0 ms. All writes so far took: 0 ms.\n",
      "[2026-01-08T16:26:08]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 80 entries (2 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:08]:WARNING:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): Closing the data source\n",
      "[2026-01-08T16:26:08]:INFO:subscribe-0: Done writing 80 entries, closing data sink. Current batch writes took: 0 ms. All writes so far took: 0 ms.\n",
      "[2026-01-08T16:26:08]:INFO:Preparing Pathway computation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Phase 1 tables recreated successfully:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08T16:26:08]:INFO:subscribe-0: Done writing 2 entries, closing data sink. Current batch writes took: 0 ms. All writes so far took: 0 ms.\n",
      "[2026-01-08T16:26:08]:INFO:Preparing Pathway computation\n",
      "[2026-01-08T16:26:08]:INFO:Enter read_snapshot method with reader PosixLike\n",
      "[2026-01-08T16:26:09]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:09]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 80 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:09]:WARNING:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): Closing the data source\n",
      "[2026-01-08T16:26:09]:INFO:subscribe-0: Done writing 31 entries, closing data sink. Current batch writes took: 0 ms. All writes so far took: 0 ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â€¢ train_table: 80 rows\n",
      "   â€¢ books_table: 2 rows\n",
      "   â€¢ joined_table: 31 rows\n",
      "\n",
      "   Sample from joined_table:\n",
      "   Characters: ['Faria', 'Noirtier']\n",
      "   Novels: ['The Count of Monte Cristo']\n",
      "   Labels: {'contradict': 16, 'consistent': 15}\n",
      "\n",
      "4. Creating character profiles...\n",
      "   Created profiles for 2 characters\n",
      "   â€¢ Faria: 2 variants, 15 backstories\n",
      "   â€¢ Noirtier: 2 variants, 16 backstories\n",
      "\n",
      "5. Extracting character evidence...\n",
      "   â€¢ Faria: 46 evidence passages\n",
      "   â€¢ Noirtier: 45 evidence passages\n",
      "\n",
      "6. Creating Pathway evidence table...\n",
      "   Evidence table created with 91 rows\n",
      "\n",
      "7. Creating statistics table...\n",
      "\n",
      "8. Creating joined evidence-backstory table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-08T16:26:12]:INFO:Preparing Pathway computation\n",
      "[2026-01-08T16:26:12]:INFO:Enter read_snapshot method with reader PosixLike\n",
      "[2026-01-08T16:26:12]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 0 entries (1 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:12]:INFO:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): 80 entries (2 minibatch(es)) have been sent to the engine\n",
      "[2026-01-08T16:26:12]:WARNING:FileSystem(/root/DataDivas_KDSH_2026/Data/train.csv): Closing the data source\n",
      "[2026-01-08T16:26:13]:INFO:subscribe-0: Done writing 1410 entries, closing data sink. Current batch writes took: 0 ms. All writes so far took: 0 ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Joined table created with 1410 rows\n",
      "\n",
      "9. Saving outputs and generating report...\n",
      "\n",
      "================================================================================\n",
      "PHASE 2 COMPLETE - SUCCESSFULLY BUILT FROM PHASE 1\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ RESULTS SUMMARY:\n",
      "   Total evidence passages: 91\n",
      "   Unique characters covered: 2\n",
      "   Average passages per character: 45.5\n",
      "\n",
      "ðŸ“Š EVIDENCE QUALITY:\n",
      "   Passages with causal context: 64.8%\n",
      "   Passages with emotional context: 48.4%\n",
      "   Character as active subject: 0.0%\n",
      "   Average narrative importance: 32.4%\n",
      "   Average backstory relevance: 55.0%\n",
      "\n",
      "ðŸ“ EVIDENCE TYPE DISTRIBUTION:\n",
      "   â€¢ Action: 38 (41.8%)\n",
      "   â€¢ Internal: 4 (4.4%)\n",
      "   â€¢ Negative: 49 (53.8%)\n",
      "\n",
      "ðŸ’¾ OUTPUTS SAVED TO: /root/DataDivas_KDSH_2026/phase2_output\n",
      "   1. character_evidence.csv - 91 evidence passages\n",
      "   2. character_statistics.csv - Statistics per character\n",
      "   3. evidence_with_backstory.csv - Joined with Phase 1 backstories\n",
      "   4. quality_report.json - Comprehensive quality metrics\n",
      "\n",
      "ðŸ”— PATHWAY TABLES CREATED:\n",
      "   1. character_evidence_table - All extracted evidence\n",
      "   2. character_statistics_table - Character-level statistics\n",
      "   3. evidence_with_backstory - Joined with original backstories\n",
      "\n",
      "ðŸš€ READY FOR PHASE 3:\n",
      "   The evidence is now structured for contradiction detection.\n",
      "   Each passage has causal/emotional context, narrative importance,\n",
      "   backstory relevance, and temporal positioning.\n",
      "\n",
      "================================================================================\n",
      "Character evidence successfully extracted with causal and emotional context preserved.\n",
      "Phase 3 can now analyze backstory consistency using this rich evidence set.\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ SAMPLE EVIDENCE (first 3 passages):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Character: Faria\n",
      "Type: NEGATIVE | Importance: 0.55\n",
      "Causal markers: so\n",
      "Emotional markers: ire|anger\n",
      "Preview: \n",
      "\n",
      "The vague disquietude which prevailed among the spectators had so much\n",
      "affected one of the crowd that he did not await the arrival of the\n",
      "vessel in harbor, but jumping into a small skiff, desired to...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Character: Faria\n",
      "Type: ACTION | Importance: 0.30\n",
      "Causal markers: then\n",
      "Emotional markers: dread\n",
      "Preview: \n",
      "\n",
      "â€œWhat happened to him?â€ asked the owner, with an air of considerable\n",
      "resignation. â€œWhat happened to the worthy captain?â€\n",
      "\n",
      "â€œHe died.â€\n",
      "\n",
      "â€œFell into the sea?â€\n",
      "\n",
      "â€œNo, sir, he died of brain-fever in dreadf...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Character: Faria\n",
      "Type: ACTION | Importance: 0.30\n",
      "Causal markers: then\n",
      "Emotional markers: ire\n",
      "Preview: \n",
      "\n",
      "All hands obeyed, and at once the eight or ten seamen who composed the\n",
      "crew, sprang to their respective stations at the spanker brails and\n",
      "outhaul, topsail sheets and halyards, the jib downhaul, and...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 02_character_context.ipynb\n",
    "\n",
    "# Phase 2: Character Context Extraction & Evidence Construction\n",
    "# Builds directly from Phase 1 outputs\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: CHARACTER CONTEXT EXTRACTION & EVIDENCE CONSTRUCTION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Goal: Isolate narrative evidence relevant to character development\")\n",
    "print(\"      while preserving causal and emotional context.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Recreate Phase 1 Environment\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n1. Recreating Phase 1 environment...\")\n",
    "\n",
    "# First, install Pathway if needed\n",
    "!pip install pathway pandas numpy -q\n",
    "\n",
    "import pathway as pw\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the same paths as Phase 1\n",
    "PROJECT_ROOT = Path(\"/root/DataDivas_KDSH_2026\")\n",
    "DATA_DIR = PROJECT_ROOT / \"Data\"\n",
    "BOOKS_DIR = DATA_DIR / \"Books\"\n",
    "\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   Data directory: {DATA_DIR}\")\n",
    "print(f\"   Books directory: {BOOKS_DIR}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Recreate Phase 1 Tables EXACTLY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n2. Recreating Phase 1 tables...\")\n",
    "\n",
    "# Recreate the exact same schema from Phase 1\n",
    "class TrainSchema(pw.Schema):\n",
    "    uid: int\n",
    "    book_name: str\n",
    "    char: str\n",
    "    caption: str\n",
    "    content: str\n",
    "    label: str\n",
    "\n",
    "class BooksSchema(pw.Schema):\n",
    "    title: str\n",
    "    full_text: str\n",
    "    file_path: str\n",
    "    char_count: int\n",
    "    word_count: int\n",
    "\n",
    "# Load training data (same as Phase 1)\n",
    "train_csv_path = str(DATA_DIR / \"train.csv\")\n",
    "test_csv_path = str(DATA_DIR / \"test.csv\")\n",
    "\n",
    "train_table = pw.io.csv.read(\n",
    "    train_csv_path,\n",
    "    schema=TrainSchema,\n",
    "    mode=\"static\"\n",
    ")\n",
    "\n",
    "test_table = pw.io.csv.read(\n",
    "    test_csv_path,\n",
    "    schema=TrainSchema,\n",
    "    mode='static'\n",
    ")\n",
    "\n",
    "# Load books (same as Phase 1)\n",
    "def load_book_content(file_path: str) -> dict:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return {\n",
    "        \"title\": Path(file_path).stem,\n",
    "        \"full_text\": content,\n",
    "        \"file_path\": file_path,\n",
    "        \"char_count\": len(content),\n",
    "        \"word_count\": len(content.split())\n",
    "    }\n",
    "\n",
    "book_data_list = []\n",
    "for book_file in BOOKS_DIR.glob(\"*.txt\"):\n",
    "    book_info = load_book_content(str(book_file))\n",
    "    book_data_list.append(book_info)\n",
    "    print(f\"   Loaded: {book_info['title']} - {book_info['word_count']:,} words\")\n",
    "\n",
    "books_df = pd.DataFrame(book_data_list)\n",
    "books_table = pw.debug.table_from_pandas(\n",
    "    books_df,\n",
    "    schema=BooksSchema\n",
    ")\n",
    "\n",
    "# Create joined_table (EXACTLY as in Phase 1)\n",
    "joined_table = train_table.join(\n",
    "    books_table,\n",
    "    pw.this.book_name == books_table.title\n",
    ").select(\n",
    "    sample_id=pw.this.uid,\n",
    "    character_name=pw.this.char,\n",
    "    backstory=pw.this.content,\n",
    "    label=pw.this.label,\n",
    "    novel_title=pw.this.book_name,\n",
    "    full_text=books_table.full_text,\n",
    "    char_count=books_table.char_count,\n",
    "    word_count=books_table.word_count\n",
    ")\n",
    "\n",
    "print(f\"\\n3. Phase 1 tables recreated successfully:\")\n",
    "train_df = pw.debug.table_to_pandas(train_table)\n",
    "books_pd = pw.debug.table_to_pandas(books_table)\n",
    "joined_df = pw.debug.table_to_pandas(joined_table)\n",
    "\n",
    "print(f\"   â€¢ train_table: {len(train_df)} rows\")\n",
    "print(f\"   â€¢ books_table: {len(books_pd)} rows\")\n",
    "print(f\"   â€¢ joined_table: {len(joined_df)} rows\")\n",
    "\n",
    "# Display sample of joined data\n",
    "print(f\"\\n   Sample from joined_table:\")\n",
    "print(f\"   Characters: {joined_df['character_name'].unique().tolist()}\")\n",
    "print(f\"   Novels: {joined_df['novel_title'].unique().tolist()}\")\n",
    "print(f\"   Labels: {joined_df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Character Profile Creation\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n4. Creating character profiles...\")\n",
    "\n",
    "@dataclass\n",
    "class CharacterProfile:\n",
    "    \"\"\"Store character identification information\"\"\"\n",
    "    primary_name: str\n",
    "    name_variants: List[str]\n",
    "    pronouns: List[str]\n",
    "    titles: List[str]\n",
    "    novel_title: str\n",
    "    novel_text: str\n",
    "    backstories: List[str]  # Multiple backstories from training data\n",
    "    labels: List[int]       # Corresponding labels\n",
    "\n",
    "class CharacterIdentifier:\n",
    "    \"\"\"Handle character name ambiguity and variant identification\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_name_variants(character_name: str) -> List[str]:\n",
    "        \"\"\"Extract possible name variants from a character name\"\"\"\n",
    "        variants = set()\n",
    "        \n",
    "        # Add full name\n",
    "        clean_name = character_name.strip()\n",
    "        variants.add(clean_name)\n",
    "        \n",
    "        # Split by common separators\n",
    "        parts = re.split(r'[\\s,\\.\\-]+', clean_name)\n",
    "        \n",
    "        # Add first name only\n",
    "        if len(parts) > 0 and len(parts[0]) > 1:\n",
    "            variants.add(parts[0])\n",
    "            # Add common abbreviations\n",
    "            if len(parts[0]) > 2:\n",
    "                variants.add(parts[0][0] + \".\")  # E. for Edmond\n",
    "            \n",
    "        # Add last name only if exists\n",
    "        if len(parts) > 1 and len(parts[-1]) > 1:\n",
    "            variants.add(parts[-1])\n",
    "            \n",
    "        # Add common French/English variants for known characters\n",
    "        name_lower = clean_name.lower()\n",
    "        if \"edmond\" in name_lower or \"dantÃ¨s\" in name_lower:\n",
    "            variants.update([\"Edmond\", \"DantÃ¨s\", \"Dantes\", \"Count\", \"Monte Cristo\"])\n",
    "        if \"paganel\" in name_lower:\n",
    "            variants.update([\"Paganel\", \"Jacques\", \"Professor\"])\n",
    "            \n",
    "        return [v for v in variants if v and len(v) > 1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def infer_pronouns(character_name: str, novel_text: str) -> List[str]:\n",
    "        \"\"\"Infer pronouns from context\"\"\"\n",
    "        if not novel_text or len(novel_text) < 1000:\n",
    "            # Default based on name patterns\n",
    "            if any(female in character_name.lower() for female in ['mary', 'anne', 'elizabeth', 'sophie']):\n",
    "                return ['she', 'her', 'hers', 'herself']\n",
    "            else:\n",
    "                return ['he', 'him', 'his', 'himself']\n",
    "        \n",
    "        # Sample text for analysis\n",
    "        sample_size = min(5000, len(novel_text))\n",
    "        samples = [\n",
    "            novel_text[:sample_size],\n",
    "            novel_text[-sample_size:] if len(novel_text) > sample_size else \"\"\n",
    "        ]\n",
    "        \n",
    "        male_indicators = {' he ', ' him ', ' his ', ' himself ', ' mr ', ' monsieur ', ' sir '}\n",
    "        female_indicators = {' she ', ' her ', ' hers ', ' herself ', ' mrs ', ' madame ', ' lady ', ' miss '}\n",
    "        \n",
    "        male_score = 0\n",
    "        female_score = 0\n",
    "        \n",
    "        for sample in samples:\n",
    "            if not sample:\n",
    "                continue\n",
    "            sample_lower = ' ' + sample.lower() + ' '\n",
    "            male_score += sum(sample_lower.count(' ' + indicator + ' ') for indicator in male_indicators)\n",
    "            female_score += sum(sample_lower.count(' ' + indicator + ' ') for indicator in female_indicators)\n",
    "        \n",
    "        # Check for gendered titles near character mentions\n",
    "        first_name = character_name.split()[0].lower() if ' ' in character_name else character_name.lower()\n",
    "        for i in range(0, min(10000, len(novel_text)), 1000):\n",
    "            snippet = novel_text[i:i+1000].lower()\n",
    "            if first_name in snippet:\n",
    "                if ' mr ' in snippet or ' monsieur ' in snippet or ' sir ' in snippet:\n",
    "                    male_score += 2\n",
    "                if ' mrs ' in snippet or ' madame ' in snippet or ' lady ' in snippet:\n",
    "                    female_score += 2\n",
    "        \n",
    "        if female_score > male_score:\n",
    "            return ['she', 'her', 'hers', 'herself']\n",
    "        else:\n",
    "            return ['he', 'him', 'his', 'himself']\n",
    "    \n",
    "    @staticmethod\n",
    "    def infer_titles(character_name: str, novel_text: str) -> List[str]:\n",
    "        \"\"\"Infer titles from context\"\"\"\n",
    "        if not novel_text:\n",
    "            return []\n",
    "        \n",
    "        titles = set()\n",
    "        first_name = character_name.split()[0] if ' ' in character_name else character_name\n",
    "        \n",
    "        # Look for patterns in first 20000 characters\n",
    "        search_text = novel_text[:20000].lower()\n",
    "        \n",
    "        # Pattern: \"the [title] [name]\"\n",
    "        patterns = [\n",
    "            rf'the ([a-z]+) {re.escape(first_name.lower())}',\n",
    "            rf'([A-Z][a-z]+) {re.escape(first_name)}',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, search_text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                title = match.group(1).lower()\n",
    "                # Filter out common words that aren't titles\n",
    "                if len(title) > 3 and title not in {'said', 'asked', 'replied', 'exclaimed'}:\n",
    "                    titles.add(f\"the {title}\")\n",
    "        \n",
    "        # Add common titles based on character type\n",
    "        name_lower = character_name.lower()\n",
    "        if any(word in name_lower for word in ['count', 'lord', 'duke', 'earl']):\n",
    "            titles.add(\"the count\")\n",
    "        if 'professor' in name_lower or 'doctor' in name_lower:\n",
    "            titles.add(\"the professor\")\n",
    "            titles.add(\"the doctor\")\n",
    "        if 'captain' in name_lower:\n",
    "            titles.add(\"the captain\")\n",
    "            \n",
    "        return list(titles)[:5]\n",
    "\n",
    "# Build comprehensive character profiles\n",
    "character_profiles = {}\n",
    "for _, row in joined_df.iterrows():\n",
    "    char_name = row['character_name']\n",
    "    novel_title = row['novel_title']\n",
    "    novel_text = row['full_text']\n",
    "    backstory = row['backstory']\n",
    "    label = str(row['label'])  # Convert to Python int\n",
    "    \n",
    "    if char_name not in character_profiles:\n",
    "        identifier = CharacterIdentifier()\n",
    "        name_variants = identifier.extract_name_variants(char_name)\n",
    "        pronouns = identifier.infer_pronouns(char_name, novel_text)\n",
    "        titles = identifier.infer_titles(char_name, novel_text)\n",
    "        \n",
    "        profile = CharacterProfile(\n",
    "            primary_name=char_name,\n",
    "            name_variants=name_variants,\n",
    "            pronouns=pronouns,\n",
    "            titles=titles,\n",
    "            novel_title=novel_title,\n",
    "            novel_text=novel_text,\n",
    "            backstories=[backstory],\n",
    "            labels=[label]\n",
    "        )\n",
    "        character_profiles[char_name] = profile\n",
    "    else:\n",
    "        # Add additional backstory if this character appears multiple times\n",
    "        if backstory not in character_profiles[char_name].backstories:\n",
    "            character_profiles[char_name].backstories.append(backstory)\n",
    "            character_profiles[char_name].labels.append(label)\n",
    "\n",
    "print(f\"   Created profiles for {len(character_profiles)} characters\")\n",
    "for char, profile in list(character_profiles.items())[:5]:  # Show first 5\n",
    "    print(f\"   â€¢ {char}: {len(profile.name_variants)} variants, {len(profile.backstories)} backstories\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Advanced Context Extraction\n",
    "# ============================================\n",
    "\n",
    "@dataclass\n",
    "class ExtractedEvidence:\n",
    "    \"\"\"Store extracted evidence with metadata\"\"\"\n",
    "    evidence_id: str\n",
    "    character_name: str\n",
    "    novel_title: str\n",
    "    passage_text: str\n",
    "    passage_start: int\n",
    "    passage_end: int\n",
    "    context_window: int\n",
    "    evidence_type: str\n",
    "    causal_markers: List[str]\n",
    "    emotional_markers: List[str]\n",
    "    temporal_position: float\n",
    "    has_pronoun: bool\n",
    "    has_direct_name: bool\n",
    "    character_role: str\n",
    "    narrative_importance: float\n",
    "    backstory_relevance: float  # How relevant to known backstories\n",
    "\n",
    "class ContextExtractor:\n",
    "    \"\"\"Intelligent context extraction with backstory awareness\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict = None):\n",
    "        self.config = config or {}\n",
    "        self.min_window = self.config.get('min_window', 150)\n",
    "        self.max_window = self.config.get('max_window', 1200)\n",
    "        self.target_window = self.config.get('target_window', 500)\n",
    "        \n",
    "        # Comprehensive marker sets\n",
    "        self.causal_markers = {\n",
    "            'because', 'since', 'therefore', 'thus', 'so', 'hence', 'consequently',\n",
    "            'as a result', 'due to', 'owing to', 'for this reason', 'accordingly',\n",
    "            'then', 'ergo', 'wherefore'\n",
    "        }\n",
    "        \n",
    "        self.emotional_markers = {\n",
    "            'fear', 'afraid', 'terrified', 'scared', 'frightened', 'dread',\n",
    "            'love', 'adore', 'cherish', 'treasure', 'passion', 'affection',\n",
    "            'hate', 'despise', 'loathe', 'abhor', 'detest', 'resent',\n",
    "            'anger', 'fury', 'rage', 'wrath', 'ire', 'outrage',\n",
    "            'joy', 'happiness', 'delight', 'elation', 'ecstasy', 'bliss',\n",
    "            'sadness', 'sorrow', 'grief', 'melancholy', 'despair', 'misery',\n",
    "            'hope', 'optimism', 'expectation', 'anticipation',\n",
    "            'regret', 'remorse', 'guilt', 'shame', 'contrition',\n",
    "            'surprise', 'amazement', 'astonishment', 'wonder'\n",
    "        }\n",
    "        \n",
    "        self.action_verbs = {\n",
    "            'ran', 'walked', 'jumped', 'fled', 'chased', 'fought', 'attacked',\n",
    "            'spoke', 'said', 'shouted', 'whispered', 'asked', 'replied', 'answered',\n",
    "            'looked', 'saw', 'watched', 'observed', 'noticed', 'glanced',\n",
    "            'thought', 'considered', 'decided', 'chose', 'determined', 'resolved',\n",
    "            'felt', 'experienced', 'sensed', 'perceived', 'realized',\n",
    "            'gave', 'took', 'received', 'accepted', 'refused', 'rejected',\n",
    "            'created', 'built', 'destroyed', 'saved', 'killed', 'helped',\n",
    "            'traveled', 'went', 'came', 'arrived', 'departed', 'left'\n",
    "        }\n",
    "        \n",
    "        self.negative_markers = {\n",
    "            'refused', 'avoided', 'rejected', 'denied', 'declined',\n",
    "            'would not', 'could not', 'did not', 'never', 'no', 'not',\n",
    "            'failed', 'unable', 'incapable', 'impossible', 'unwilling',\n",
    "            'refrained', 'abstained', 'withheld', 'resisted'\n",
    "        }\n",
    "        \n",
    "        self.internal_markers = {\n",
    "            'thought', 'wondered', 'considered', 'pondered', 'mused',\n",
    "            'felt', 'believed', 'knew', 'understood', 'realized',\n",
    "            'remembered', 'recalled', 'recollected', 'reflected',\n",
    "            'wished', 'hoped', 'dreamed', 'imagined', 'feared'\n",
    "        }\n",
    "    \n",
    "    def find_character_mentions(self, text: str, profile: CharacterProfile) -> List[Tuple[int, int, str, float]]:\n",
    "        \"\"\"Find all mentions with position, type, and confidence\"\"\"\n",
    "        mentions = []\n",
    "        \n",
    "        # Look for name variants with word boundaries\n",
    "        for variant in profile.name_variants:\n",
    "            if len(variant) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Escape special regex characters\n",
    "            escaped = re.escape(variant)\n",
    "            pattern = re.compile(rf'\\b{escaped}\\b', re.IGNORECASE)\n",
    "            \n",
    "            for match in pattern.finditer(text):\n",
    "                confidence = 1.0 if variant == profile.primary_name else 0.8\n",
    "                # Higher confidence if capitalized (likely proper noun)\n",
    "                if match.group()[0].isupper():\n",
    "                    confidence += 0.1\n",
    "                mentions.append((match.start(), match.end(), 'name', confidence))\n",
    "        \n",
    "        # Look for pronouns with context awareness\n",
    "        if len(text) > 0:\n",
    "            for pronoun in profile.pronouns:\n",
    "                pattern = re.compile(rf'\\b{pronoun}\\b', re.IGNORECASE)\n",
    "                for match in pattern.finditer(text):\n",
    "                    # Analyze context window\n",
    "                    context_start = max(0, match.start() - 100)\n",
    "                    context_end = min(len(text), match.end() + 100)\n",
    "                    context = text[context_start:context_end].lower()\n",
    "                    \n",
    "                    # Check for other character mentions\n",
    "                    other_character_count = 0\n",
    "                    for other_char, other_profile in character_profiles.items():\n",
    "                        if other_char != profile.primary_name:\n",
    "                            for other_variant in other_profile.name_variants[:3]:\n",
    "                                if other_variant.lower() in context:\n",
    "                                    other_character_count += 1\n",
    "                                    break\n",
    "                    \n",
    "                    confidence = max(0.3, 0.7 - (other_character_count * 0.15))\n",
    "                    mentions.append((match.start(), match.end(), 'pronoun', confidence))\n",
    "        \n",
    "        # Look for titles\n",
    "        for title in profile.titles:\n",
    "            pattern = re.compile(rf'\\b{re.escape(title)}\\b', re.IGNORECASE)\n",
    "            for match in pattern.finditer(text):\n",
    "                mentions.append((match.start(), match.end(), 'title', 0.6))\n",
    "        \n",
    "        # Sort and deduplicate\n",
    "        mentions.sort(key=lambda x: x[0])\n",
    "        \n",
    "        filtered = []\n",
    "        current_end = -50  # Allow small overlap\n",
    "        \n",
    "        for mention in mentions:\n",
    "            start, end, mtype, confidence = mention\n",
    "            if start >= current_end:\n",
    "                filtered.append(mention)\n",
    "                current_end = end\n",
    "            else:\n",
    "                # Overlap - keep higher confidence\n",
    "                prev_start, prev_end, prev_type, prev_conf = filtered[-1]\n",
    "                if confidence > prev_conf:\n",
    "                    filtered[-1] = mention\n",
    "                    current_end = end\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def calculate_backstory_relevance(self, passage: str, profile: CharacterProfile) -> float:\n",
    "        \"\"\"Calculate how relevant passage is to known backstories\"\"\"\n",
    "        if not profile.backstories:\n",
    "            return 0.5  # Neutral if no backstory\n",
    "        \n",
    "        passage_lower = passage.lower()\n",
    "        relevance_score = 0.0\n",
    "        \n",
    "        # Check for backstory keywords in passage\n",
    "        backstory_keywords = set()\n",
    "        for backstory in profile.backstories:\n",
    "            # Extract meaningful words from backstory\n",
    "            words = re.findall(r'\\b[a-zA-Z]{4,}\\b', backstory.lower())\n",
    "            backstory_keywords.update(words)\n",
    "        \n",
    "        # Count keyword matches\n",
    "        for keyword in backstory_keywords:\n",
    "            if keyword in passage_lower:\n",
    "                relevance_score += 0.1\n",
    "        \n",
    "        # Check for thematic consistency\n",
    "        themes = {\n",
    "            'revenge': ['revenge', 'vengeance', 'retribution'],\n",
    "            'love': ['love', 'affection', 'devotion'],\n",
    "            'betrayal': ['betray', 'treachery', 'deception'],\n",
    "            'redemption': ['redemption', 'forgiveness', 'salvation'],\n",
    "            'ambition': ['ambition', 'goal', 'purpose']\n",
    "        }\n",
    "        \n",
    "        for theme, keywords in themes.items():\n",
    "            for keyword in keywords:\n",
    "                if keyword in passage_lower:\n",
    "                    relevance_score += 0.05\n",
    "        \n",
    "        return min(1.0, relevance_score)\n",
    "    \n",
    "    def extract_passage_with_context(self, text: str, mention_positions: List[Tuple[int, int]], \n",
    "                                    profile: CharacterProfile) -> Optional[ExtractedEvidence]:\n",
    "        \"\"\"Extract a meaningful passage around mentions\"\"\"\n",
    "        if not mention_positions:\n",
    "            return None\n",
    "        \n",
    "        # Calculate center of mentions\n",
    "        positions = [pos for start, end, _, _ in mention_positions for pos in (start, end)]\n",
    "        if not positions:\n",
    "            return None\n",
    "            \n",
    "        center = int(np.mean(positions))\n",
    "        \n",
    "        # Initial window\n",
    "        start = max(0, center - self.min_window // 2)\n",
    "        end = min(len(text), center + self.min_window // 2)\n",
    "        \n",
    "        # Expand to natural boundaries\n",
    "        start = self.find_previous_boundary(text, start)\n",
    "        end = self.find_next_boundary(text, end)\n",
    "        \n",
    "        # Ensure we don't exceed max window\n",
    "        if end - start > self.max_window:\n",
    "            # Center the window\n",
    "            start = max(0, center - self.max_window // 2)\n",
    "            end = min(len(text), center + self.max_window // 2)\n",
    "            start = self.find_previous_boundary(text, start)\n",
    "            end = self.find_next_boundary(text, end)\n",
    "        \n",
    "        passage = text[start:end]\n",
    "        if len(passage) < 100:  # Too short to be meaningful\n",
    "            return None\n",
    "        \n",
    "        # Analyze passage\n",
    "        analysis = self.analyze_passage(passage, profile)\n",
    "        \n",
    "        # Skip if low quality\n",
    "        if analysis['narrative_importance'] < 0.1:\n",
    "            return None\n",
    "        \n",
    "        # Create evidence ID\n",
    "        evidence_id = f\"{profile.primary_name}_{start}_{end}_{hash(passage[:50]) & 0xffffffff}\"\n",
    "        \n",
    "        return ExtractedEvidence(\n",
    "            evidence_id=evidence_id,\n",
    "            character_name=profile.primary_name,\n",
    "            novel_title=profile.novel_title,\n",
    "            passage_text=passage,\n",
    "            passage_start=start,\n",
    "            passage_end=end,\n",
    "            context_window=end - start,\n",
    "            evidence_type=analysis['evidence_type'],\n",
    "            causal_markers=analysis['causal_markers'],\n",
    "            emotional_markers=analysis['emotional_markers'],\n",
    "            temporal_position=float(start) / len(text) if len(text) > 0 else 0.0,\n",
    "            has_pronoun=any('pronoun' in m[2] for m in mention_positions),\n",
    "            has_direct_name=any('name' in m[2] for m in mention_positions),\n",
    "            character_role=analysis['character_role'],\n",
    "            narrative_importance=analysis['narrative_importance'],\n",
    "            backstory_relevance=self.calculate_backstory_relevance(passage, profile)\n",
    "        )\n",
    "    \n",
    "    def find_previous_boundary(self, text: str, position: int) -> int:\n",
    "        \"\"\"Find previous sentence or paragraph boundary\"\"\"\n",
    "        # Look for paragraph boundary first\n",
    "        for i in range(position, max(0, position - 500), -1):\n",
    "            if i > 1 and text[i-2:i] == '\\n\\n':\n",
    "                return max(0, i-2)\n",
    "        \n",
    "        # Look for sentence boundary\n",
    "        for i in range(position, max(0, position - 300), -1):\n",
    "            if i > 0 and text[i-1] in '.!?' and (i == len(text) or text[i] in ' \\n\\t'):\n",
    "                return max(0, i)\n",
    "        \n",
    "        return max(0, position - 200)\n",
    "    \n",
    "    def find_next_boundary(self, text: str, position: int) -> int:\n",
    "        \"\"\"Find next sentence or paragraph boundary\"\"\"\n",
    "        # Look for paragraph boundary first\n",
    "        for i in range(position, min(len(text), position + 500)):\n",
    "            if i + 2 <= len(text) and text[i:i+2] == '\\n\\n':\n",
    "                return min(len(text), i+2)\n",
    "        \n",
    "        # Look for sentence boundary\n",
    "        for i in range(position, min(len(text), position + 300)):\n",
    "            if text[i] in '.!?' and (i+1 >= len(text) or text[i+1] in ' \\n\\t'):\n",
    "                return min(len(text), i+1)\n",
    "        \n",
    "        return min(len(text), position + 200)\n",
    "    \n",
    "    def analyze_passage(self, passage: str, profile: CharacterProfile) -> Dict:\n",
    "        \"\"\"Analyze passage content comprehensively\"\"\"\n",
    "        passage_lower = passage.lower()\n",
    "        char_name_lower = profile.primary_name.lower()\n",
    "        \n",
    "        # Count markers\n",
    "        action_count = sum(1 for verb in self.action_verbs if f' {verb} ' in passage_lower)\n",
    "        dialogue_count = passage.count('\"') + passage.count(\"'\")\n",
    "        internal_count = sum(passage_lower.count(f' {marker} ') for marker in self.internal_markers)\n",
    "        negative_count = sum(1 for marker in self.negative_markers if f' {marker} ' in passage_lower)\n",
    "        \n",
    "        # Find causal markers\n",
    "        causal_found = [marker for marker in self.causal_markers if marker in passage_lower]\n",
    "        \n",
    "        # Find emotional markers\n",
    "        emotional_found = [marker for marker in self.emotional_markers if marker in passage_lower]\n",
    "        \n",
    "        # Determine character role (subject vs object/mentioned)\n",
    "        char_role = 'mentioned'\n",
    "        sentences = re.split(r'[.!?]+', passage)\n",
    "        for sentence in sentences[:3]:  # Check first few sentences\n",
    "            sentence_lower = sentence.lower()\n",
    "            # Check if character name is in sentence\n",
    "            name_in_sentence = any(variant.lower() in sentence_lower for variant in profile.name_variants[:3])\n",
    "            \n",
    "            if name_in_sentence:\n",
    "                # Simple grammar check: name followed by verb\n",
    "                words = sentence.strip().split()\n",
    "                for i, word in enumerate(words):\n",
    "                    word_lower = word.lower()\n",
    "                    # Check if this word contains a character name variant\n",
    "                    for variant in profile.name_variants[:3]:\n",
    "                        if variant.lower() in word_lower:\n",
    "                            # Check next word for verb\n",
    "                            if i < len(words) - 1:\n",
    "                                next_word = words[i+1].lower()\n",
    "                                if any(verb in next_word for verb in self.action_verbs):\n",
    "                                    char_role = 'subject'\n",
    "                                    break\n",
    "                    if char_role == 'subject':\n",
    "                        break\n",
    "            if char_role == 'subject':\n",
    "                break\n",
    "        \n",
    "        # Calculate narrative importance score\n",
    "        importance = 0.0\n",
    "        \n",
    "        # Base importance\n",
    "        importance += min(action_count * 0.05, 0.2)\n",
    "        importance += min(dialogue_count * 0.02, 0.1)\n",
    "        importance += min(internal_count * 0.1, 0.3)\n",
    "        importance += min(len(causal_found) * 0.15, 0.3)\n",
    "        importance += min(len(emotional_found) * 0.1, 0.2)\n",
    "        \n",
    "        # Bonus for character as subject\n",
    "        if char_role == 'subject':\n",
    "            importance += 0.15\n",
    "        \n",
    "        # Bonus for negative evidence (rare but important)\n",
    "        if negative_count > 0:\n",
    "            importance += 0.1\n",
    "        \n",
    "        importance = min(importance, 1.0)\n",
    "        \n",
    "        # Determine evidence type\n",
    "        type_scores = {\n",
    "            'action': action_count,\n",
    "            'dialogue': dialogue_count * 3,  # Weight dialogue more\n",
    "            'internal': internal_count * 2,  # Weight internal more\n",
    "            'negative': negative_count * 5   # Weight negative evidence highest\n",
    "        }\n",
    "        \n",
    "        evidence_type = max(type_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        return {\n",
    "            'evidence_type': evidence_type,\n",
    "            'causal_markers': causal_found,\n",
    "            'emotional_markers': emotional_found,\n",
    "            'character_role': char_role,\n",
    "            'narrative_importance': float(importance),\n",
    "            'action_count': int(action_count),\n",
    "            'dialogue_count': int(dialogue_count),\n",
    "            'internal_count': int(internal_count),\n",
    "            'negative_count': int(negative_count)\n",
    "        }\n",
    "    \n",
    "    def extract_evidence_for_profile(self, profile: CharacterProfile) -> List[ExtractedEvidence]:\n",
    "        \"\"\"Extract all evidence for a character profile\"\"\"\n",
    "        if not profile.novel_text or len(profile.novel_text) < 1000:\n",
    "            return []\n",
    "        \n",
    "        evidence_list = []\n",
    "        text = profile.novel_text\n",
    "        \n",
    "        # Find all mentions\n",
    "        mentions = self.find_character_mentions(text, profile)\n",
    "        \n",
    "        if not mentions:\n",
    "            return []\n",
    "        \n",
    "        # Group nearby mentions\n",
    "        groups = []\n",
    "        current_group = []\n",
    "        group_threshold = 300  # characters\n",
    "        \n",
    "        for mention in mentions:\n",
    "            if not current_group:\n",
    "                current_group.append(mention)\n",
    "            else:\n",
    "                last_end = current_group[-1][1]\n",
    "                if mention[0] - last_end < group_threshold:\n",
    "                    current_group.append(mention)\n",
    "                else:\n",
    "                    if current_group:\n",
    "                        groups.append(current_group)\n",
    "                    current_group = [mention]\n",
    "        \n",
    "        if current_group:\n",
    "            groups.append(current_group)\n",
    "        \n",
    "        # Extract passages for each group\n",
    "        for group in groups[:50]:  # Limit to 50 groups per character\n",
    "            evidence = self.extract_passage_with_context(text, group, profile)\n",
    "            if evidence:\n",
    "                evidence_list.append(evidence)\n",
    "        \n",
    "        return evidence_list\n",
    "\n",
    "print(\"\\n5. Extracting character evidence...\")\n",
    "\n",
    "# Configure and run extractor\n",
    "extractor_config = {\n",
    "    'min_window': 200,\n",
    "    'max_window': 1500,\n",
    "    'target_window': 600\n",
    "}\n",
    "extractor = ContextExtractor(extractor_config)\n",
    "\n",
    "# Extract evidence for all characters\n",
    "all_evidence = []\n",
    "for char_name, profile in character_profiles.items():\n",
    "    evidence = extractor.extract_evidence_for_profile(profile)\n",
    "    all_evidence.extend(evidence)\n",
    "    print(f\"   â€¢ {char_name}: {len(evidence)} evidence passages\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: Create Pathway Evidence Table\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n6. Creating Pathway evidence table...\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "evidence_data = []\n",
    "for ev in all_evidence:\n",
    "    evidence_data.append({\n",
    "        'evidence_id': ev.evidence_id,\n",
    "        'character_name': ev.character_name,\n",
    "        'novel_title': ev.novel_title,\n",
    "        'passage_text': ev.passage_text[:10000],  # Limit text length\n",
    "        'passage_start': int(ev.passage_start),\n",
    "        'passage_end': int(ev.passage_end),\n",
    "        'context_window': int(ev.context_window),\n",
    "        'evidence_type': ev.evidence_type,\n",
    "        'causal_markers': '|'.join(ev.causal_markers[:10]),  # Limit markers\n",
    "        'emotional_markers': '|'.join(ev.emotional_markers[:10]),\n",
    "        'temporal_position': float(ev.temporal_position),\n",
    "        'has_pronoun': bool(ev.has_pronoun),\n",
    "        'has_direct_name': bool(ev.has_direct_name),\n",
    "        'character_role': ev.character_role,\n",
    "        'narrative_importance': float(ev.narrative_importance),\n",
    "        'backstory_relevance': float(ev.backstory_relevance),\n",
    "        'passage_length': int(len(ev.passage_text))\n",
    "    })\n",
    "\n",
    "evidence_df = pd.DataFrame(evidence_data)\n",
    "\n",
    "# Create Pathway schema\n",
    "class EvidenceSchema(pw.Schema):\n",
    "    evidence_id: str\n",
    "    character_name: str\n",
    "    novel_title: str\n",
    "    passage_text: str\n",
    "    passage_start: int\n",
    "    passage_end: int\n",
    "    context_window: int\n",
    "    evidence_type: str\n",
    "    causal_markers: str\n",
    "    emotional_markers: str\n",
    "    temporal_position: float\n",
    "    has_pronoun: bool\n",
    "    has_direct_name: bool\n",
    "    character_role: str\n",
    "    narrative_importance: float\n",
    "    backstory_relevance: float\n",
    "    passage_length: int\n",
    "\n",
    "# Create Pathway table\n",
    "character_evidence_table = pw.debug.table_from_pandas(evidence_df, schema=EvidenceSchema)\n",
    "\n",
    "print(f\"   Evidence table created with {len(evidence_df)} rows\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 6: Create Statistics Table\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n7. Creating statistics table...\")\n",
    "\n",
    "# Calculate statistics\n",
    "stats_data = []\n",
    "for char_name in evidence_df['character_name'].unique():\n",
    "    char_ev = evidence_df[evidence_df['character_name'] == char_name]\n",
    "    \n",
    "    if len(char_ev) == 0:\n",
    "        continue\n",
    "    \n",
    "    stats = {\n",
    "        'character_name': char_name,\n",
    "        'total_passages': int(len(char_ev)),\n",
    "        'avg_passage_length': float(char_ev['passage_length'].mean()),\n",
    "        'total_chars_extracted': int(char_ev['passage_length'].sum()),\n",
    "        'action_count': int(len(char_ev[char_ev['evidence_type'] == 'action'])),\n",
    "        'dialogue_count': int(len(char_ev[char_ev['evidence_type'] == 'dialogue'])),\n",
    "        'internal_count': int(len(char_ev[char_ev['evidence_type'] == 'internal'])),\n",
    "        'negative_count': int(len(char_ev[char_ev['evidence_type'] == 'negative'])),\n",
    "        'with_causal': int((char_ev['causal_markers'] != '').sum()),\n",
    "        'with_emotional': int((char_ev['emotional_markers'] != '').sum()),\n",
    "        'temporal_range': float(char_ev['temporal_position'].max() - char_ev['temporal_position'].min()),\n",
    "        'avg_importance': float(char_ev['narrative_importance'].mean()),\n",
    "        'avg_backstory_relevance': float(char_ev['backstory_relevance'].mean()),\n",
    "        'pronoun_ratio': float(char_ev['has_pronoun'].mean()),\n",
    "        'direct_name_ratio': float(char_ev['has_direct_name'].mean()),\n",
    "        'subject_ratio': float((char_ev['character_role'] == 'subject').mean())\n",
    "    }\n",
    "    stats_data.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "class StatsSchema(pw.Schema):\n",
    "    character_name: str\n",
    "    total_passages: int\n",
    "    avg_passage_length: float\n",
    "    total_chars_extracted: int\n",
    "    action_count: int\n",
    "    dialogue_count: int\n",
    "    internal_count: int\n",
    "    negative_count: int\n",
    "    with_causal: int\n",
    "    with_emotional: int\n",
    "    temporal_range: float\n",
    "    avg_importance: float\n",
    "    avg_backstory_relevance: float\n",
    "    pronoun_ratio: float\n",
    "    direct_name_ratio: float\n",
    "    subject_ratio: float\n",
    "\n",
    "character_statistics_table = pw.debug.table_from_pandas(stats_df, schema=StatsSchema)\n",
    "\n",
    "# ============================================\n",
    "# STEP 7: Create Joined Evidence-Backstory Table\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n8. Creating joined evidence-backstory table...\")\n",
    "\n",
    "# Join evidence with original backstories\n",
    "evidence_with_backstory = joined_table.join(\n",
    "    character_evidence_table,\n",
    "    joined_table.character_name == character_evidence_table.character_name\n",
    ").select(\n",
    "    sample_id=joined_table.sample_id,\n",
    "    character_name=joined_table.character_name,\n",
    "    backstory=joined_table.backstory,\n",
    "    label=joined_table.label,\n",
    "    novel_title=joined_table.novel_title,\n",
    "    evidence_id=character_evidence_table.evidence_id,\n",
    "    evidence_text=character_evidence_table.passage_text,\n",
    "    evidence_type=character_evidence_table.evidence_type,\n",
    "    causal_markers=character_evidence_table.causal_markers,\n",
    "    emotional_markers=character_evidence_table.emotional_markers,\n",
    "    temporal_position=character_evidence_table.temporal_position,\n",
    "    character_role=character_evidence_table.character_role,\n",
    "    narrative_importance=character_evidence_table.narrative_importance,\n",
    "    backstory_relevance=character_evidence_table.backstory_relevance\n",
    ")\n",
    "\n",
    "# Convert to pandas for inspection\n",
    "joined_evidence_df = pw.debug.table_to_pandas(evidence_with_backstory)\n",
    "print(f\"   Joined table created with {len(joined_evidence_df)} rows\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 8: Save Outputs and Generate Report\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n9. Saving outputs and generating report...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = PROJECT_ROOT / \"phase2_output\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Convert numpy types to Python native types for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"Convert numpy types to Python native types for JSON serialization\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Save DataFrames\n",
    "evidence_df.to_csv(output_dir / \"character_evidence.csv\", index=False)\n",
    "stats_df.to_csv(output_dir / \"character_statistics.csv\", index=False)\n",
    "joined_evidence_df.to_csv(output_dir / \"evidence_with_backstory.csv\", index=False)\n",
    "\n",
    "# Generate quality report\n",
    "total_evidence = len(evidence_df)\n",
    "total_characters = evidence_df['character_name'].nunique()\n",
    "\n",
    "quality_report = {\n",
    "    'phase': 'Phase 2 - Character Context Extraction',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'summary': {\n",
    "        'total_evidence_passages': int(total_evidence),\n",
    "        'unique_characters': int(total_characters),\n",
    "        'avg_passages_per_character': float(total_evidence / total_characters if total_characters > 0 else 0),\n",
    "        'total_chars_extracted': int(evidence_df['passage_length'].sum()),\n",
    "        'avg_passage_length': float(evidence_df['passage_length'].mean())\n",
    "    },\n",
    "    'evidence_quality': {\n",
    "        'causal_context_present': float((evidence_df['causal_markers'] != '').mean()),\n",
    "        'emotional_context_present': float((evidence_df['emotional_markers'] != '').mean()),\n",
    "        'character_as_subject': float((evidence_df['character_role'] == 'subject').mean()),\n",
    "        'avg_narrative_importance': float(evidence_df['narrative_importance'].mean()),\n",
    "        'avg_backstory_relevance': float(evidence_df['backstory_relevance'].mean())\n",
    "    },\n",
    "    'evidence_type_distribution': convert_to_serializable(evidence_df['evidence_type'].value_counts().to_dict()),\n",
    "    'extraction_config': extractor_config,\n",
    "    'character_coverage': convert_to_serializable({\n",
    "        char: int(len(evidence_df[evidence_df['character_name'] == char])) \n",
    "        for char in evidence_df['character_name'].unique()\n",
    "    }),\n",
    "    'acknowledged_limitations': [\n",
    "        \"Pronoun resolution without full coreference\",\n",
    "        \"Limited handling of name ambiguity across characters\",\n",
    "        \"Simplified grammatical role detection\",\n",
    "        \"Fixed context window sizes\",\n",
    "        \"Basic backstory relevance scoring\"\n",
    "    ],\n",
    "    'success_metrics_achieved': [\n",
    "        \"Temporal ordering preserved via position tracking\",\n",
    "        \"Causal markers explicitly extracted\",\n",
    "        \"Emotional context captured\",\n",
    "        \"Negative evidence specifically identified\",\n",
    "        \"Character actions prioritized\",\n",
    "        \"Backstory relevance calculated\",\n",
    "        \"Comprehensive metadata for Phase 3\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save report\n",
    "with open(output_dir / \"quality_report.json\", 'w') as f:\n",
    "    json.dump(convert_to_serializable(quality_report), f, indent=2)\n",
    "\n",
    "# ============================================\n",
    "# STEP 9: Final Summary\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2 COMPLETE - SUCCESSFULLY BUILT FROM PHASE 1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ RESULTS SUMMARY:\")\n",
    "print(f\"   Total evidence passages: {total_evidence}\")\n",
    "print(f\"   Unique characters covered: {total_characters}\")\n",
    "if total_characters > 0:\n",
    "    print(f\"   Average passages per character: {total_evidence/total_characters:.1f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š EVIDENCE QUALITY:\")\n",
    "print(f\"   Passages with causal context: {quality_report['evidence_quality']['causal_context_present']*100:.1f}%\")\n",
    "print(f\"   Passages with emotional context: {quality_report['evidence_quality']['emotional_context_present']*100:.1f}%\")\n",
    "print(f\"   Character as active subject: {quality_report['evidence_quality']['character_as_subject']*100:.1f}%\")\n",
    "print(f\"   Average narrative importance: {quality_report['evidence_quality']['avg_narrative_importance']*100:.1f}%\")\n",
    "print(f\"   Average backstory relevance: {quality_report['evidence_quality']['avg_backstory_relevance']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“ EVIDENCE TYPE DISTRIBUTION:\")\n",
    "evidence_dist = quality_report['evidence_type_distribution']\n",
    "for ev_type in ['action', 'dialogue', 'internal', 'negative']:\n",
    "    if ev_type in evidence_dist:\n",
    "        count = evidence_dist[ev_type]\n",
    "        percentage = (count / total_evidence) * 100\n",
    "        print(f\"   â€¢ {ev_type.title()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ OUTPUTS SAVED TO: {output_dir}\")\n",
    "print(f\"   1. character_evidence.csv - {total_evidence} evidence passages\")\n",
    "print(f\"   2. character_statistics.csv - Statistics per character\")\n",
    "print(f\"   3. evidence_with_backstory.csv - Joined with Phase 1 backstories\")\n",
    "print(f\"   4. quality_report.json - Comprehensive quality metrics\")\n",
    "\n",
    "print(f\"\\nðŸ”— PATHWAY TABLES CREATED:\")\n",
    "print(f\"   1. character_evidence_table - All extracted evidence\")\n",
    "print(f\"   2. character_statistics_table - Character-level statistics\")\n",
    "print(f\"   3. evidence_with_backstory - Joined with original backstories\")\n",
    "\n",
    "print(f\"\\nðŸš€ READY FOR PHASE 3:\")\n",
    "print(f\"   The evidence is now structured for contradiction detection.\")\n",
    "print(f\"   Each passage has causal/emotional context, narrative importance,\")\n",
    "print(f\"   backstory relevance, and temporal positioning.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Character evidence successfully extracted with causal and emotional context preserved.\")\n",
    "print(\"Phase 3 can now analyze backstory consistency using this rich evidence set.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display sample evidence\n",
    "if len(evidence_df) > 0:\n",
    "    print(\"\\nðŸ“ SAMPLE EVIDENCE (first 3 passages):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, row in evidence_df.head(3).iterrows():\n",
    "        print(f\"\\nCharacter: {row['character_name']}\")\n",
    "        print(f\"Type: {row['evidence_type'].upper()} | Importance: {row['narrative_importance']:.2f}\")\n",
    "        print(f\"Causal markers: {row['causal_markers'] if row['causal_markers'] else 'None'}\")\n",
    "        print(f\"Emotional markers: {row['emotional_markers'] if row['emotional_markers'] else 'None'}\")\n",
    "        print(f\"Preview: {row['passage_text'][:200]}...\")\n",
    "        print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
